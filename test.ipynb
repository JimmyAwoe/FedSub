{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90ac76fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from pickle import dump\n",
    "import os \n",
    "#os.environ['PYTORCH_NO_CUDA_MEMORY_CACHING'] = '1'\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:21\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b7127ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Coordinate_descend_genep(dim, comp_dim):\n",
    "    assert dim >= comp_dim, \"compression dimension must be smaller than dimension\"\n",
    "    ide = torch.eye(dim, requires_grad=False)\n",
    "    select_col = torch.randperm(dim)[:comp_dim]\n",
    "    sign = torch.randint(0, 2, (comp_dim, ))\n",
    "    sign = sign * 2 - 1\n",
    "    # XXX make clear whether PTP is I or PPT is I\n",
    "    P = ide[:, select_col] * sign\n",
    "    return P\n",
    "\n",
    "def mem():\n",
    "    #torch.cuda.empty_cache()\n",
    "    print('memory allocated: ' + str((torch.cuda.memory_allocated() / 1024)) )\n",
    "    print('memory reserved: ' + str(torch.cuda.memory_reserved() / 1024) )\n",
    "    print('max memory allocated: ' + str(torch.cuda.max_memory_allocated() / 1024) )\n",
    "    print('max memory reserved: ' + str(torch.cuda.max_memory_reserved() / 1024) )\n",
    "\n",
    "class SubScafLinearTest(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear network with compressed dimension\n",
    "    \"\"\"\n",
    "    def __init__(self, comp_dim: int, comp_mat: torch.Tensor, wraped_model: nn.Linear):\n",
    "        self.comp_mat = comp_mat\n",
    "        self.comp_dim = comp_dim\n",
    "        device = wraped_model.weight.device\n",
    "        dtype = wraped_model.weight.dtype\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        self.x = wraped_model.weight.detach().clone()\n",
    "        super().__init__()\n",
    "        self.b = nn.Parameter(torch.zeros((comp_dim, wraped_model.in_features), **factory_kwargs))\n",
    "    \n",
    "    def comp_mul(self, b):\n",
    "        return self.comp_mat @ b + self.x\n",
    "\n",
    "    def forward(self, input):\n",
    "        #def compute_linear(input):\n",
    "            #weight = self.comp_mat @ self.b + self.x\n",
    "            #return F.linear(input, weight)\n",
    "        #return checkpoint(compute_linear, input, use_reentrant=False)\n",
    "        mem()\n",
    "        x = F.linear(self.comp_mat, self.b.T)\n",
    "        print(x.grad_fn)\n",
    "        mem()\n",
    "        x = self.x + x\n",
    "        print(x.grad_fn)\n",
    "        mem()\n",
    "        output = F.linear(input, x)\n",
    "        print(output.grad_fn)\n",
    "        mem()\n",
    "        return output \n",
    "\n",
    "class SubScafLayer(nn.Module):\n",
    "    def __init__(self, comp_dim, comp_mat, wraped_model):\n",
    "        super().__init__()\n",
    "        self.layer1 = SubScafLinearTest(comp_dim, comp_mat, wraped_model)\n",
    "        self.layer2 = SubScafLinearTest(comp_dim, comp_mat, wraped_model)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = checkpoint(self.layer1, input)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a4f4ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dim = 64\n",
    "wraped_module = nn.Linear(512, 512, bias=False)\n",
    "comp_mat = Coordinate_descend_genep(wraped_module.out_features, comp_dim)\n",
    "model = SubScafLinearTest(comp_dim, comp_mat, wraped_module)\n",
    "\n",
    "activation_values = []\n",
    "def hook(module, input, output):\n",
    "    activation_values.append(output.detach().clone())\n",
    "handle = model.register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09bf3adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 3328.0\n",
      "<MmBackward0 object at 0x78015159fe80>\n",
      "memory allocated: 12672.0\n",
      "<AddBackward0 object at 0x78015159fe50>\n",
      "memory allocated: 12672.0\n",
      "<MmBackward0 object at 0x78023ebacd30>\n",
      "memory allocated: 13696.0\n",
      "memory allocated: 12672.0\n"
     ]
    }
   ],
   "source": [
    "#torch.cuda.memory._record_memory_history(enabled='all')\n",
    "device = \"cuda:0\"\n",
    "comp_dim = 64\n",
    "wraped_module = nn.Linear(512, 512, bias=False).to(device)\n",
    "comp_mat = Coordinate_descend_genep(wraped_module.out_features, comp_dim).to(device)\n",
    "model = SubScafLinearTest(comp_dim, comp_mat, wraped_module).to(device)\n",
    "input_data = torch.eye(512).to(device)\n",
    "output = model(input_data)\n",
    "mem()\n",
    "output.sum().backward()\n",
    "opt = torch.optim.SGD(model.parameters())\n",
    "opt.zero_grad()\n",
    "#s = torch.cuda.memory._snapshot()\n",
    "#with open(f\"snapshot.pickle\", \"wb\") as f:\n",
    "    #dump(s, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c82f0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 19968.0\n",
      "memory allocated: 19072.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output = model(input_data)\n",
    "mem()\n",
    "output.sum().backward()\n",
    "del output\n",
    "mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ea97959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 2432.0\n",
      "memory reserved: 4096.0\n",
      "max memory allocated: 3456.0\n",
      "max memory reserved: 4096.0\n",
      "memory allocated: 3456.0\n",
      "memory reserved: 4096.0\n",
      "max memory allocated: 3456.0\n",
      "max memory reserved: 4096.0\n",
      "memory allocated: 3456.0\n",
      "memory reserved: 4096.0\n",
      "max memory allocated: 3456.0\n",
      "max memory reserved: 4096.0\n",
      "None\n",
      "memory allocated: 12800.0\n",
      "memory reserved: 26624.0\n",
      "max memory allocated: 12800.0\n",
      "max memory reserved: 26624.0\n",
      "None\n",
      "memory allocated: 12800.0\n",
      "memory reserved: 26624.0\n",
      "max memory allocated: 13824.0\n",
      "max memory reserved: 26624.0\n",
      "None\n",
      "memory allocated: 13824.0\n",
      "memory reserved: 26624.0\n",
      "max memory allocated: 13824.0\n",
      "max memory reserved: 26624.0\n",
      "memory allocated: 12800.0\n",
      "memory reserved: 26624.0\n",
      "max memory allocated: 13824.0\n",
      "max memory reserved: 26624.0\n",
      "<MmBackward0 object at 0x7ec61479b6d0>\n",
      "memory allocated: 13824.0\n",
      "memory reserved: 26624.0\n",
      "max memory allocated: 13824.0\n",
      "max memory reserved: 26624.0\n",
      "<AddBackward0 object at 0x7ec61479a170>\n",
      "memory allocated: 13824.0\n",
      "memory reserved: 28672.0\n",
      "max memory allocated: 14848.0\n",
      "max memory reserved: 28672.0\n",
      "<MmBackward0 object at 0x7ec6147b66b0>\n",
      "memory allocated: 14848.0\n",
      "memory reserved: 28672.0\n",
      "max memory allocated: 14848.0\n",
      "max memory reserved: 28672.0\n",
      "memory allocated: 13824.0\n",
      "memory reserved: 28672.0\n",
      "max memory allocated: 14848.0\n",
      "max memory reserved: 28672.0\n",
      "memory allocated: 21248.0\n",
      "memory reserved: 28672.0\n",
      "max memory allocated: 24193.0\n",
      "max memory reserved: 28672.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#torch.cuda.memory._record_memory_history(enabled='all')\n",
    "device = \"cuda:0\"\n",
    "comp_dim = 64\n",
    "wraped_module = nn.Linear(512, 512, bias=False).to(device)\n",
    "comp_mat = Coordinate_descend_genep(wraped_module.out_features, comp_dim).to(device)\n",
    "model = SubScafLayer(comp_dim, comp_mat, wraped_module).to(device)\n",
    "del wraped_module\n",
    "mem()\n",
    "input_data = torch.eye(512).to(device)\n",
    "mem()\n",
    "output = model(input_data)\n",
    "mem()\n",
    "#del output\n",
    "output.sum().backward()\n",
    "mem()\n",
    "#s = torch.cuda.memory._snapshot()\n",
    "#with open(f\"snapshot.pickle\", \"wb\") as f:\n",
    "    #dump(s, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6930b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 0.0\n",
      "memory reserved: 0.0\n",
      "max memory allocated: 0.0\n",
      "max memory reserved: 0.0\n",
      "memory allocated: 4.0\n",
      "memory reserved: 2048.0\n",
      "max memory allocated: 4.0\n",
      "max memory reserved: 2048.0\n",
      "memory allocated: 4.5\n",
      "memory reserved: 2048.0\n",
      "max memory allocated: 4.5\n",
      "max memory reserved: 2048.0\n",
      "memory allocated: 8325.0\n",
      "memory reserved: 22528.0\n",
      "max memory allocated: 8325.0\n",
      "max memory reserved: 22528.0\n",
      "memory allocated: 16649.0\n",
      "memory reserved: 22528.0\n",
      "max memory allocated: 16650.5\n",
      "max memory reserved: 22528.0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "mem()\n",
    "model = nn.Linear(32, 32, bias=False).to(device)\n",
    "mem()\n",
    "input_data = torch.randn((1, 32)).to(device)\n",
    "mem()\n",
    "output = model(input_data)\n",
    "#with torch.no_grad():\n",
    "    #output = model(input_data)\n",
    "mem()\n",
    "\n",
    "output.sum().backward()\n",
    "\n",
    "mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abbd6bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Error while downloading from https://hf-mirror.com/openai-community/gpt2/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    LlamaConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "import os \n",
    "os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "#model_config = AutoConfig.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d245807c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f17fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def Coordinate_descend_genep(d, r, n=1):\n",
    "    sum_p = np.zeros((d, r)) \n",
    "    for _ in range(n):\n",
    "        ide = np.eye(d)\n",
    "        col_num = np.arange(d)\n",
    "        select_col = np.random.choice(col_num, r, replace=False)\n",
    "        sign = np.random.choice([-1, 1], r)\n",
    "        P = np.sqrt(d / r) * ide[:, select_col] * sign\n",
    "        #P = ide[:, select_col] * sign\n",
    "        sum_p += P\n",
    "    P = sum_p / n\n",
    "    return P\n",
    "\n",
    "\n",
    "def Spherical_smoothing_genep(d, r, n=1):\n",
    "    sum_p = np.zeros((d, r)) \n",
    "    for _ in range(n):\n",
    "        z = np.random.randn(d, d)\n",
    "        Q, R = np.linalg.qr(z)\n",
    "        D = np.diag(np.sign(np.diag(R)))\n",
    "        Q = Q @ D\n",
    "        R = D @ R\n",
    "        assert np.allclose(Q @ R, z, atol=1e-7), \"the QR decomposion is not accuracy\"\n",
    "        #P = np.sqrt(d / r) * Q[:, :r]\n",
    "        P = Q[:, :r]\n",
    "        sum_p += P\n",
    "    P = sum_p / n\n",
    "    return P\n",
    "\n",
    "def gene_random_matrix(in_dim, out_dim):\n",
    "    return np.random.randn(in_dim, out_dim) / np.sqrt(out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e2c4335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "worker_num = 20\n",
    "total_sample_size = 1000\n",
    "d = 10\n",
    "def genHeLS(total_sample_size, d, worker_num):\n",
    "    assert total_sample_size % worker_num == 0, \"please make sure total_sample can be divided for each worker equally\"\n",
    "    mean = np.random.uniform(low=-1, high=1, size=worker_num)\n",
    "    scale = np.sqrt(np.random.uniform(low=0.5, high=1.5, size=worker_num))\n",
    "    a_list = []\n",
    "    for i in range(worker_num):\n",
    "        a_list.append(np.random.normal(loc=mean[i], scale=scale[i], size=(total_sample_size // worker_num, d)))\n",
    "    a = np.concatenate(a_list, axis=0)\n",
    "    ans = np.random.randn(d, 1)\n",
    "    b = a @ ans + 0.1 * np.random.randn(total_sample_size, 1)\n",
    "    return a, b\n",
    "\n",
    "def solLS(A, b):\n",
    "    x_sol = np.linalg.inv(A.T@A)@(A.T@b)\n",
    "    print(A.T@(A@x_sol-b) / len(A))\n",
    "    return x_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cff5cf6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 0., 0., 0.],\n",
       "       [0., 2., 0., 0., 0.],\n",
       "       [0., 0., 2., 0., 0.],\n",
       "       [0., 0., 0., 2., 0.],\n",
       "       [0., 0., 0., 0., 2.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcd = Coordinate_descend_genep(10, 5)\n",
    "pss = Spherical_smoothing_genep(10, 5)\n",
    "prd = gene_random_matrix(10, 5)\n",
    "\n",
    "pcd.T @ pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31a51229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07551295, -0.43366106,  0.36777972,  0.63740934,  0.45284414])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(prd, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a229b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0., -0., -1., -0., -0., -0., -0., -0., -0., -0.],\n",
       "        [-0., -0., -0., -0., -0., -0., -1., -0., -0., -0.],\n",
       "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -1.],\n",
       "        [-0., -0., -0., -0., -0., -0., -0., -0., -1., -0.],\n",
       "        [-0., -0., -0., -0., -1., -0., -0., -0., -0., -0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from utils import Coordinate_descend_genep\n",
    "def set_seed(seed):\n",
    "    # Set random seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "#set_seed(42)\n",
    "Coordinate_descend_genep(5, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
