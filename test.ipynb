{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90ac76fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from pickle import dump\n",
    "import os \n",
    "#os.environ['PYTORCH_NO_CUDA_MEMORY_CACHING'] = '1'\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:21\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b7127ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Coordinate_descend_genep(dim, comp_dim):\n",
    "    assert dim >= comp_dim, \"compression dimension must be smaller than dimension\"\n",
    "    ide = torch.eye(dim, requires_grad=False)\n",
    "    select_col = torch.randperm(dim)[:comp_dim]\n",
    "    sign = torch.randint(0, 2, (comp_dim, ))\n",
    "    sign = sign * 2 - 1\n",
    "    # XXX make clear whether PTP is I or PPT is I\n",
    "    P = ide[:, select_col] * sign\n",
    "    return P\n",
    "\n",
    "def mem():\n",
    "    #torch.cuda.empty_cache()\n",
    "    print('memory allocated: ' + str((torch.cuda.memory_allocated() / 1024)) )\n",
    "    print('memory reserved: ' + str(torch.cuda.memory_reserved() / 1024) )\n",
    "    print('max memory allocated: ' + str(torch.cuda.max_memory_allocated() / 1024) )\n",
    "    print('max memory reserved: ' + str(torch.cuda.max_memory_reserved() / 1024) )\n",
    "\n",
    "class SubScafLinearTest(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear network with compressed dimension\n",
    "    \"\"\"\n",
    "    def __init__(self, comp_dim: int, comp_mat: torch.Tensor, wraped_model: nn.Linear):\n",
    "        self.comp_mat = comp_mat\n",
    "        self.comp_dim = comp_dim\n",
    "        device = wraped_model.weight.device\n",
    "        dtype = wraped_model.weight.dtype\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        self.x = wraped_model.weight.detach().clone()\n",
    "        super().__init__()\n",
    "        self.b = nn.Parameter(torch.zeros((comp_dim, wraped_model.in_features), **factory_kwargs))\n",
    "    \n",
    "    def comp_mul(self, b):\n",
    "        return self.comp_mat @ b + self.x\n",
    "\n",
    "    def forward(self, input):\n",
    "        #def compute_linear(input):\n",
    "            #weight = self.comp_mat @ self.b + self.x\n",
    "            #return F.linear(input, weight)\n",
    "        #return checkpoint(compute_linear, input, use_reentrant=False)\n",
    "        mem()\n",
    "        x = F.linear(self.comp_mat, self.b.T)\n",
    "        print(x.grad_fn)\n",
    "        mem()\n",
    "        x = self.x + x\n",
    "        print(x.grad_fn)\n",
    "        mem()\n",
    "        output = F.linear(input, x)\n",
    "        print(output.grad_fn)\n",
    "        mem()\n",
    "        return output \n",
    "\n",
    "class SubScafLayer(nn.Module):\n",
    "    def __init__(self, comp_dim, comp_mat, wraped_model):\n",
    "        super().__init__()\n",
    "        self.layer1 = SubScafLinearTest(comp_dim, comp_mat, wraped_model)\n",
    "        self.layer2 = SubScafLinearTest(comp_dim, comp_mat, wraped_model)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = checkpoint(self.layer1, input)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2375376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_compiled_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'b',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'comp_dim',\n",
       " 'comp_mat',\n",
       " 'comp_mul',\n",
       " 'compile',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'ipu',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'x',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a4f4ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dim = 64\n",
    "wraped_module = nn.Linear(512, 512, bias=False)\n",
    "comp_mat = Coordinate_descend_genep(wraped_module.out_features, comp_dim)\n",
    "model = SubScafLinearTest(comp_dim, comp_mat, wraped_module)\n",
    "\n",
    "activation_values = []\n",
    "def hook(module, input, output):\n",
    "    activation_values.append(output.detach().clone())\n",
    "handle = model.register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09bf3adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 3328.0\n",
      "<MmBackward0 object at 0x78015159fe80>\n",
      "memory allocated: 12672.0\n",
      "<AddBackward0 object at 0x78015159fe50>\n",
      "memory allocated: 12672.0\n",
      "<MmBackward0 object at 0x78023ebacd30>\n",
      "memory allocated: 13696.0\n",
      "memory allocated: 12672.0\n"
     ]
    }
   ],
   "source": [
    "#torch.cuda.memory._record_memory_history(enabled='all')\n",
    "device = \"cuda:0\"\n",
    "comp_dim = 64\n",
    "wraped_module = nn.Linear(512, 512, bias=False).to(device)\n",
    "comp_mat = Coordinate_descend_genep(wraped_module.out_features, comp_dim).to(device)\n",
    "model = SubScafLinearTest(comp_dim, comp_mat, wraped_module).to(device)\n",
    "input_data = torch.eye(512).to(device)\n",
    "output = model(input_data)\n",
    "mem()\n",
    "output.sum().backward()\n",
    "opt = torch.optim.SGD(model.parameters())\n",
    "opt.zero_grad()\n",
    "#s = torch.cuda.memory._snapshot()\n",
    "#with open(f\"snapshot.pickle\", \"wb\") as f:\n",
    "    #dump(s, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c82f0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 19968.0\n",
      "memory allocated: 19072.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output = model(input_data)\n",
    "mem()\n",
    "output.sum().backward()\n",
    "del output\n",
    "mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ea97959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 2432.0\n",
      "memory reserved: 4096.0\n",
      "max memory allocated: 3456.0\n",
      "max memory reserved: 4096.0\n",
      "memory allocated: 3456.0\n",
      "memory reserved: 4096.0\n",
      "max memory allocated: 3456.0\n",
      "max memory reserved: 4096.0\n",
      "memory allocated: 3456.0\n",
      "memory reserved: 4096.0\n",
      "max memory allocated: 3456.0\n",
      "max memory reserved: 4096.0\n",
      "None\n",
      "memory allocated: 12800.0\n",
      "memory reserved: 26624.0\n",
      "max memory allocated: 12800.0\n",
      "max memory reserved: 26624.0\n",
      "None\n",
      "memory allocated: 12800.0\n",
      "memory reserved: 26624.0\n",
      "max memory allocated: 13824.0\n",
      "max memory reserved: 26624.0\n",
      "None\n",
      "memory allocated: 13824.0\n",
      "memory reserved: 26624.0\n",
      "max memory allocated: 13824.0\n",
      "max memory reserved: 26624.0\n",
      "memory allocated: 12800.0\n",
      "memory reserved: 26624.0\n",
      "max memory allocated: 13824.0\n",
      "max memory reserved: 26624.0\n",
      "<MmBackward0 object at 0x7ec61479b6d0>\n",
      "memory allocated: 13824.0\n",
      "memory reserved: 26624.0\n",
      "max memory allocated: 13824.0\n",
      "max memory reserved: 26624.0\n",
      "<AddBackward0 object at 0x7ec61479a170>\n",
      "memory allocated: 13824.0\n",
      "memory reserved: 28672.0\n",
      "max memory allocated: 14848.0\n",
      "max memory reserved: 28672.0\n",
      "<MmBackward0 object at 0x7ec6147b66b0>\n",
      "memory allocated: 14848.0\n",
      "memory reserved: 28672.0\n",
      "max memory allocated: 14848.0\n",
      "max memory reserved: 28672.0\n",
      "memory allocated: 13824.0\n",
      "memory reserved: 28672.0\n",
      "max memory allocated: 14848.0\n",
      "max memory reserved: 28672.0\n",
      "memory allocated: 21248.0\n",
      "memory reserved: 28672.0\n",
      "max memory allocated: 24193.0\n",
      "max memory reserved: 28672.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#torch.cuda.memory._record_memory_history(enabled='all')\n",
    "device = \"cuda:0\"\n",
    "comp_dim = 64\n",
    "wraped_module = nn.Linear(512, 512, bias=False).to(device)\n",
    "comp_mat = Coordinate_descend_genep(wraped_module.out_features, comp_dim).to(device)\n",
    "model = SubScafLayer(comp_dim, comp_mat, wraped_module).to(device)\n",
    "del wraped_module\n",
    "mem()\n",
    "input_data = torch.eye(512).to(device)\n",
    "mem()\n",
    "output = model(input_data)\n",
    "mem()\n",
    "#del output\n",
    "output.sum().backward()\n",
    "mem()\n",
    "#s = torch.cuda.memory._snapshot()\n",
    "#with open(f\"snapshot.pickle\", \"wb\") as f:\n",
    "    #dump(s, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6930b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 0.0\n",
      "memory reserved: 0.0\n",
      "max memory allocated: 0.0\n",
      "max memory reserved: 0.0\n",
      "memory allocated: 4.0\n",
      "memory reserved: 2048.0\n",
      "max memory allocated: 4.0\n",
      "max memory reserved: 2048.0\n",
      "memory allocated: 4.5\n",
      "memory reserved: 2048.0\n",
      "max memory allocated: 4.5\n",
      "max memory reserved: 2048.0\n",
      "memory allocated: 8325.0\n",
      "memory reserved: 22528.0\n",
      "max memory allocated: 8325.0\n",
      "max memory reserved: 22528.0\n",
      "memory allocated: 16649.0\n",
      "memory reserved: 22528.0\n",
      "max memory allocated: 16650.5\n",
      "max memory reserved: 22528.0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "mem()\n",
    "model = nn.Linear(32, 32, bias=False).to(device)\n",
    "mem()\n",
    "input_data = torch.randn((1, 32)).to(device)\n",
    "mem()\n",
    "output = model(input_data)\n",
    "#with torch.no_grad():\n",
    "    #output = model(input_data)\n",
    "mem()\n",
    "\n",
    "output.sum().backward()\n",
    "\n",
    "mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0324364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfaa493",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd6bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
