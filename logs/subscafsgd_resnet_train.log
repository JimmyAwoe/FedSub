W0717 11:53:15.992000 560703 site-packages/torch/distributed/run.py:766] 
W0717 11:53:15.992000 560703 site-packages/torch/distributed/run.py:766] *****************************************
W0717 11:53:15.992000 560703 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0717 11:53:15.992000 560703 site-packages/torch/distributed/run.py:766] *****************************************
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/xuyuqi/SubScaf/resnet.py", line 341, in <module>
[rank2]:     main(args)
[rank2]:   File "/home/xuyuqi/SubScaf/resnet.py", line 58, in main
[rank2]:     model.to(device)
[rank2]:   File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1355, in to
[rank2]:     return self._apply(convert)
[rank2]:   File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/nn/modules/module.py", line 915, in _apply
[rank2]:     module._apply(fn)
[rank2]:   File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/nn/modules/module.py", line 942, in _apply
[rank2]:     param_applied = fn(param)
[rank2]:   File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1341, in convert
[rank2]:     return t.to(
[rank2]: RuntimeError: CUDA error: invalid device ordinal
[rank2]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[rank2]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[rank2]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[rank7]: Traceback (most recent call last):
[rank7]:   File "/home/xuyuqi/SubScaf/resnet.py", line 341, in <module>
[rank7]:     main(args)
[rank7]:   File "/home/xuyuqi/SubScaf/resnet.py", line 58, in main
[rank7]:     model.to(device)
[rank7]:   File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1355, in to
[rank7]:     return self._apply(convert)
[rank7]:   File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/nn/modules/module.py", line 915, in _apply
[rank7]:     module._apply(fn)
[rank7]:   File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/nn/modules/module.py", line 942, in _apply
[rank7]:     param_applied = fn(param)
[rank7]:   File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1341, in convert
[rank7]:     return t.to(
[rank7]: RuntimeError: CUDA error: invalid device ordinal
[rank7]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[rank7]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[rank7]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[rank6]: Traceback (most recent call last):
[rank6]:   File "/home/xuyuqi/SubScaf/resnet.py", line 341, in <module>
[rank6]:     main(args)
[rank6]:   File "/home/xuyuqi/SubScaf/resnet.py", line 58, in main
[rank6]:     model.to(device)
[rank6]:   File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1355, in to
[rank6]:     return self._apply(convert)
[rank6]:   File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/nn/modules/module.py", line 915, in _apply
[rank6]:     module._apply(fn)
[rank6]:   File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/nn/modules/module.py", line 942, in _apply
[rank6]:     param_applied = fn(param)
[rank6]:   File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1341, in convert
[rank6]:     return t.to(
[rank6]: RuntimeError: CUDA error: invalid device ordinal
[rank6]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[rank6]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[rank6]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[rank5]: Traceback (most recent call last):
[rank5]:   File "/home/xuyuqi/SubScaf/resnet.py", line 341, in <module>
[rank5]:     main(args)
[rank5]:   File "/home/xuyuqi/SubScaf/resnet.py", line 58, in main
[rank5]:     model.to(device)
[rank5]:   File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1355, in to
[rank5]:     return self._apply(convert)
[rank5]:   File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/nn/modules/module.py", line 915, in _apply
[rank5]:     module._apply(fn)
[rank5]:   File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/nn/modules/module.py", line 942, in _apply
[rank5]:     param_applied = fn(param)
[rank5]:   File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1341, in convert
[rank5]:     return t.to(
[rank5]: RuntimeError: CUDA error: invalid device ordinal
[rank5]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[rank5]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[rank5]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/xuyuqi/SubScaf/resnet.py", line 341, in <module>
[rank3]:     main(args)
[rank3]:   File "/home/xuyuqi/SubScaf/resnet.py", line 58, in main
[rank3]:     model.to(device)
[rank3]:   File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1355, in to
[rank3]:     return self._apply(convert)
[rank3]:   File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/nn/modules/module.py", line 915, in _apply
[rank3]:     module._apply(fn)
[rank3]:   File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/nn/modules/module.py", line 942, in _apply
[rank3]:     param_applied = fn(param)
[rank3]:   File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1341, in convert
[rank3]:     return t.to(
[rank3]: RuntimeError: CUDA error: invalid device ordinal
[rank3]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[rank3]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[rank3]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[rank4]: Traceback (most recent call last):
[rank4]:   File "/home/xuyuqi/SubScaf/resnet.py", line 341, in <module>
[rank4]:     main(args)
[rank4]:   File "/home/xuyuqi/SubScaf/resnet.py", line 58, in main
[rank4]:     model.to(device)
[rank4]:   File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1355, in to
[rank4]:     return self._apply(convert)
[rank4]:   File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/nn/modules/module.py", line 915, in _apply
[rank4]:     module._apply(fn)
[rank4]:   File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/nn/modules/module.py", line 942, in _apply
[rank4]:     param_applied = fn(param)
[rank4]:   File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1341, in convert
[rank4]:     return t.to(
[rank4]: RuntimeError: CUDA error: invalid device ordinal
[rank4]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[rank4]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[rank4]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[rank2]:[W717 11:53:22.979112987 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank6]:[W717 11:53:22.019921349 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank4]:[W717 11:53:22.078863133 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W0717 11:53:23.524000 560703 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 560954 closing signal SIGTERM
W0717 11:53:23.524000 560703 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 560955 closing signal SIGTERM
E0717 11:53:23.853000 560703 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 2 (pid: 560956) of binary: /home/xuyuqi/.conda/envs/minimind/bin/python
Traceback (most recent call last):
  File "/home/xuyuqi/.conda/envs/minimind/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in main
    run(args)
  File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
resnet.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-07-17_11:53:23
  host      : user-NULL
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 560957)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-07-17_11:53:23
  host      : user-NULL
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 560958)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-07-17_11:53:23
  host      : user-NULL
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 560959)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2025-07-17_11:53:23
  host      : user-NULL
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 560960)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2025-07-17_11:53:23
  host      : user-NULL
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 560961)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-17_11:53:23
  host      : user-NULL
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 560956)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
