W0630 15:53:06.565000 1223363 site-packages/torch/distributed/run.py:766] 
W0630 15:53:06.565000 1223363 site-packages/torch/distributed/run.py:766] *****************************************
W0630 15:53:06.565000 1223363 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0630 15:53:06.565000 1223363 site-packages/torch/distributed/run.py:766] *****************************************
2025-06-30 15:53:12.555 | INFO     | utils.common:log:12 - Process group initialize
2025-06-30 15:53:12.555 | INFO     | utils.common:log:12 - ****************************************
2025-06-30 15:53:12.555 | INFO     | utils.common:log:12 - Start training with arguments
2025-06-30 15:53:12.555 | INFO     | utils.common:log:12 - lr                             0.001
2025-06-30 15:53:12.555 | INFO     | utils.common:log:12 - batch_size                     64
2025-06-30 15:53:12.555 | INFO     | utils.common:log:12 - total_batch_size               64
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - max_length                     1024
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - num_training_steps             3
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - grad_clip                      0.0
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - warmup                         1000
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - constant_lr                    True
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - seed                           42
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - mixed_precision                bf16
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - comp_dim                       256
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - tau                            10
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - gene_method                    cd
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - jump_certain_modules           False
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - update_cp_freq                 50
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - adaptive_cp_rate               0.0
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - model_config                   configs/llama_60m.json
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - optimizer                      subscafsgd
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - momentum                       0.0
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - dampening                      0
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - weight_decay                   0
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - nesterov                       False
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - per_layer_weight_update        False
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - use_wandb                      False
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - wandb_run_name                 real_lazy_update
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - use_tqdm                       False
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - use_log                        True
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - mem_monitor                    False
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - flash_attn                     False
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - ckpt                           True
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - measure_comm                   False
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - change_cd                      3000
2025-06-30 15:53:12.556 | INFO     | utils.common:log:12 - ****************************************
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/xyq/gitee-SubScaf/llama_pretrain.py", line 427, in <module>
[rank0]:     main(args)
[rank0]:   File "/home/xyq/gitee-SubScaf/llama_pretrain.py", line 75, in main
[rank0]:     ds = load_dataset("/data/datasets/c4_en", split="train", streaming=True)
[rank0]:   File "/home/xyq/miniconda3/envs/subscaf/lib/python3.10/site-packages/datasets/load.py", line 2606, in load_dataset
[rank0]:     builder_instance = load_dataset_builder(
[rank0]:   File "/home/xyq/miniconda3/envs/subscaf/lib/python3.10/site-packages/datasets/load.py", line 2277, in load_dataset_builder
[rank0]:     dataset_module = dataset_module_factory(
[rank0]:   File "/home/xyq/miniconda3/envs/subscaf/lib/python3.10/site-packages/datasets/load.py", line 1925, in dataset_module_factory
[rank0]:     raise FileNotFoundError(
[rank0]: FileNotFoundError: Couldn't find a dataset script at /data/datasets/c4_en/c4_en.py or any data file in the same directory.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/xyq/gitee-SubScaf/llama_pretrain.py", line 427, in <module>
[rank1]:     main(args)
[rank1]:   File "/home/xyq/gitee-SubScaf/llama_pretrain.py", line 75, in main
[rank1]:     ds = load_dataset("/data/datasets/c4_en", split="train", streaming=True)
[rank1]:   File "/home/xyq/miniconda3/envs/subscaf/lib/python3.10/site-packages/datasets/load.py", line 2606, in load_dataset
[rank1]:     builder_instance = load_dataset_builder(
[rank1]:   File "/home/xyq/miniconda3/envs/subscaf/lib/python3.10/site-packages/datasets/load.py", line 2277, in load_dataset_builder
[rank1]:     dataset_module = dataset_module_factory(
[rank1]:   File "/home/xyq/miniconda3/envs/subscaf/lib/python3.10/site-packages/datasets/load.py", line 1925, in dataset_module_factory
[rank1]:     raise FileNotFoundError(
[rank1]: FileNotFoundError: Couldn't find a dataset script at /data/datasets/c4_en/c4_en.py or any data file in the same directory.
[rank0]:[W630 15:53:14.878524810 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W0630 15:53:15.094000 1223363 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1223524 closing signal SIGTERM
E0630 15:53:15.208000 1223363 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 1223523) of binary: /home/xyq/miniconda3/envs/subscaf/bin/python3.10
Traceback (most recent call last):
  File "/home/xyq/miniconda3/envs/subscaf/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/xyq/miniconda3/envs/subscaf/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/xyq/miniconda3/envs/subscaf/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in main
    run(args)
  File "/home/xyq/miniconda3/envs/subscaf/lib/python3.10/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/home/xyq/miniconda3/envs/subscaf/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xyq/miniconda3/envs/subscaf/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
llama_pretrain.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-06-30_15:53:15
  host      : ubuntu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1223523)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
