nohup: ignoring input
W0620 19:56:26.452000 3779580 site-packages/torch/distributed/run.py:766] 
W0620 19:56:26.452000 3779580 site-packages/torch/distributed/run.py:766] *****************************************
W0620 19:56:26.452000 3779580 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0620 19:56:26.452000 3779580 site-packages/torch/distributed/run.py:766] *****************************************
2025-06-20 19:56:31.030 | INFO     | utils.common:log:12 - Process group initialize
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - ****************************************
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - Start training with arguments
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - lr                             0.001
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - batch_size                     16
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - total_batch_size               16
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - max_length                     1024
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - num_training_steps             10
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - grad_clip                      0.0
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - warmup                         1000
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - constant_lr                    True
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - seed                           42
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - mixed_precision                bf16
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - comp_dim                       64
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - tau                            5
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - gene_method                    svd
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - jump_certain_modules           False
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - update_cp_freq                 15
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - adaptive_cp_rate               0.2
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - model_config                   configs/llama_350m.json
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - optimizer                      sgd
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - momentum                       0.0
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - dampening                      0
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - weight_decay                   0
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - nesterov                       False
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - per_layer_weight_update        False
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - use_wandb                      False
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - wandb_run_name                 real_lazy_update
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - use_tqdm                       False
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - use_log                        True
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - mem_monitor                    False
2025-06-20 19:56:31.031 | INFO     | utils.common:log:12 - flash_attn                     False
2025-06-20 19:56:31.032 | INFO     | utils.common:log:12 - ckpt                           True
2025-06-20 19:56:31.032 | INFO     | utils.common:log:12 - ****************************************
/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation
  warnings.warn(
/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation
  warnings.warn(
2025-06-20 19:56:39.205 | INFO     | utils.common:log:12 - 
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 1024, padding_idx=31999)
    (layers): ModuleList(
      (0-23): 24 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=1024, out_features=2736, bias=False)
          (up_proj): Linear(in_features=1024, out_features=2736, bias=False)
          (down_proj): Linear(in_features=2736, out_features=1024, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): LlamaRMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): LlamaRMSNorm((1024,), eps=1e-06)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)
)

2025-06-20 19:56:39.206 | INFO     | utils.common:log:12 - Total params: 367.97M
2025-06-20 19:56:39.207 | INFO     | utils.common:log:12 - Trainable params: 367.97M
/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-06-20 19:56:41.356 | INFO     | utils.common:log:12 - step: 1/10 Loss: 10.5991\10.599 Lr: 1.000e-3 Mem: 31.398 GB Throughput_tokens: 14.9193
2025-06-20 19:56:42.512 | INFO     | utils.common:log:12 - step: 2/10 Loss: 10.5568\10.595 Lr: 1.000e-3 Mem: 31.406 GB Throughput_tokens: 27.7153
2025-06-20 19:56:43.666 | INFO     | utils.common:log:12 - step: 3/10 Loss: 10.5980\10.595 Lr: 1.000e-3 Mem: 31.406 GB Throughput_tokens: 27.7745
2025-06-20 19:56:44.825 | INFO     | utils.common:log:12 - step: 4/10 Loss: 10.5872\10.594 Lr: 1.000e-3 Mem: 31.406 GB Throughput_tokens: 27.6734
2025-06-20 19:56:45.981 | INFO     | utils.common:log:12 - step: 5/10 Loss: 10.5885\10.594 Lr: 1.000e-3 Mem: 31.406 GB Throughput_tokens: 27.7070
2025-06-20 19:56:47.138 | INFO     | utils.common:log:12 - step: 6/10 Loss: 10.5500\10.589 Lr: 1.000e-3 Mem: 31.406 GB Throughput_tokens: 27.6991
2025-06-20 19:56:48.293 | INFO     | utils.common:log:12 - step: 7/10 Loss: 10.5629\10.587 Lr: 1.000e-3 Mem: 31.406 GB Throughput_tokens: 27.7654
2025-06-20 19:56:49.448 | INFO     | utils.common:log:12 - step: 8/10 Loss: 10.5561\10.584 Lr: 1.000e-3 Mem: 31.406 GB Throughput_tokens: 27.7335
2025-06-20 19:56:50.606 | INFO     | utils.common:log:12 - step: 9/10 Loss: 10.5555\10.581 Lr: 1.000e-3 Mem: 31.406 GB Throughput_tokens: 27.6837
2025-06-20 19:56:51.763 | INFO     | utils.common:log:12 - step: 10/10 Loss: 10.5478\10.578 Lr: 1.000e-3 Mem: 31.406 GB Throughput_tokens: 27.7127
2025-06-20 19:56:51.763 | INFO     | utils.common:log:12 - ETA: 00:00:00 
2025-06-20 19:56:52.921 | INFO     | utils.common:log:12 - step: 11/10 Loss: 10.5381\10.574 Lr: 1.000e-3 Mem: 31.406 GB Throughput_tokens: 27.6711
Rank 1 stopping training
2025-06-20 19:56:52.926 | INFO     | utils.common:log:12 - attain assigned training step 10. Stop Training
Rank 0 stopping training
2025-06-20 19:56:52.940 | INFO     | utils.common:log:12 - finish training
2025-06-20 19:56:52.975 | INFO     | utils.common:log:12 - Total Time: 00:00:21
