W0625 15:47:00.347000 3993897 site-packages/torch/distributed/run.py:766] 
W0625 15:47:00.347000 3993897 site-packages/torch/distributed/run.py:766] *****************************************
W0625 15:47:00.347000 3993897 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0625 15:47:00.347000 3993897 site-packages/torch/distributed/run.py:766] *****************************************
2025-06-25 15:47:15.425 | INFO     | utils.common:log:12 - 
ResNet(
  (conv1): Conv2d(3, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (1): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (2): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (3): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (4): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (5): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (6): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (7): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (8): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (9): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (10): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (11): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (12): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (13): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (14): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (15): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (16): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (17): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(16, 32, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): LambdaLayer()
    )
    (1): BasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (2): BasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (3): BasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (4): BasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (5): BasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (6): BasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (7): BasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (8): BasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (9): BasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (10): BasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (11): BasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (12): BasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (13): BasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (14): BasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (15): BasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (16): BasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (17): BasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(32, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): LambdaLayer()
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (2): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (3): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (4): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (5): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (6): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (7): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (8): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (9): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (10): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (11): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (12): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (13): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (14): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (15): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (16): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (17): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (linear): Linear(in_features=64, out_features=100, bias=True)
)

2025-06-25 15:47:15.426 | INFO     | utils.common:log:12 - Total params: 9.37M
2025-06-25 15:47:15.426 | INFO     | utils.common:log:12 - Trainable params: 9.37M
2025-06-25 15:47:15.428 | INFO     | utils.common:log:12 - current lr 0.00000e+00
2025-06-25 15:47:16.197 | INFO     | utils.common:log:12 - Epoch: [0][0/391]	Loss: 11.8635	Prec@5: 3.125	lr: 0.0010
/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/autograd/graph.py:824: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/xuyuqi/.conda/envs/minimind/lib/python3.10/site-packages/torch/autograd/graph.py:824: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-06-25 15:47:16.942 | INFO     | utils.common:log:12 - Epoch: [0][10/391]	Loss: 9.4435	Prec@5: 5.824	lr: 0.0110
2025-06-25 15:47:17.479 | INFO     | utils.common:log:12 - Epoch: [0][20/391]	Loss: 8.2203	Prec@5: 5.804	lr: 0.0210
2025-06-25 15:47:18.007 | INFO     | utils.common:log:12 - Epoch: [0][30/391]	Loss: 7.0983	Prec@5: 7.611	lr: 0.0310
2025-06-25 15:47:18.521 | INFO     | utils.common:log:12 - Epoch: [0][40/391]	Loss: 6.5325	Prec@5: 8.270	lr: 0.0410
2025-06-25 15:47:19.052 | INFO     | utils.common:log:12 - Epoch: [0][50/391]	Loss: 6.2406	Prec@5: 8.977	lr: 0.0510
2025-06-25 15:47:19.575 | INFO     | utils.common:log:12 - Epoch: [0][60/391]	Loss: 6.0218	Prec@5: 9.401	lr: 0.0610
2025-06-25 15:47:20.109 | INFO     | utils.common:log:12 - Epoch: [0][70/391]	Loss: 5.8688	Prec@5: 9.771	lr: 0.0710
2025-06-25 15:47:20.649 | INFO     | utils.common:log:12 - Epoch: [0][80/391]	Loss: 5.7065	Prec@5: 10.243	lr: 0.0810
2025-06-25 15:47:21.167 | INFO     | utils.common:log:12 - Epoch: [0][90/391]	Loss: 5.6145	Prec@5: 10.319	lr: 0.0910
2025-06-25 15:47:21.690 | INFO     | utils.common:log:12 - Epoch: [0][100/391]	Loss: 5.5469	Prec@5: 10.427	lr: 0.1000
2025-06-25 15:47:22.227 | INFO     | utils.common:log:12 - Epoch: [0][110/391]	Loss: 5.4710	Prec@5: 10.740	lr: 0.1000
2025-06-25 15:47:22.767 | INFO     | utils.common:log:12 - Epoch: [0][120/391]	Loss: 5.3892	Prec@5: 10.783	lr: 0.1000
2025-06-25 15:47:23.309 | INFO     | utils.common:log:12 - Epoch: [0][130/391]	Loss: 5.3055	Prec@5: 10.938	lr: 0.0999
2025-06-25 15:47:23.854 | INFO     | utils.common:log:12 - Epoch: [0][140/391]	Loss: 5.2275	Prec@5: 11.447	lr: 0.0999
2025-06-25 15:47:24.427 | INFO     | utils.common:log:12 - Epoch: [0][150/391]	Loss: 5.1568	Prec@5: 11.703	lr: 0.0998
2025-06-25 15:47:24.983 | INFO     | utils.common:log:12 - Epoch: [0][160/391]	Loss: 5.0919	Prec@5: 12.296	lr: 0.0997
2025-06-25 15:47:25.559 | INFO     | utils.common:log:12 - Epoch: [0][170/391]	Loss: 5.0354	Prec@5: 12.902	lr: 0.0996
2025-06-25 15:47:26.139 | INFO     | utils.common:log:12 - Epoch: [0][180/391]	Loss: 4.9803	Prec@5: 13.450	lr: 0.0995
2025-06-25 15:47:26.716 | INFO     | utils.common:log:12 - Epoch: [0][190/391]	Loss: 4.9294	Prec@5: 13.899	lr: 0.0994
2025-06-25 15:47:27.303 | INFO     | utils.common:log:12 - Epoch: [0][200/391]	Loss: 4.8831	Prec@5: 14.412	lr: 0.0993
2025-06-25 15:47:27.898 | INFO     | utils.common:log:12 - Epoch: [0][210/391]	Loss: 4.8353	Prec@5: 15.033	lr: 0.0991
2025-06-25 15:47:28.500 | INFO     | utils.common:log:12 - Epoch: [0][220/391]	Loss: 4.7892	Prec@5: 15.667	lr: 0.0990
2025-06-25 15:47:29.114 | INFO     | utils.common:log:12 - Epoch: [0][230/391]	Loss: 4.7505	Prec@5: 16.234	lr: 0.0988
2025-06-25 15:47:29.728 | INFO     | utils.common:log:12 - Epoch: [0][240/391]	Loss: 4.7119	Prec@5: 16.786	lr: 0.0986
2025-06-25 15:47:30.354 | INFO     | utils.common:log:12 - Epoch: [0][250/391]	Loss: 4.6727	Prec@5: 17.337	lr: 0.0984
2025-06-25 15:47:30.988 | INFO     | utils.common:log:12 - Epoch: [0][260/391]	Loss: 4.6364	Prec@5: 17.882	lr: 0.0982
2025-06-25 15:47:31.626 | INFO     | utils.common:log:12 - Epoch: [0][270/391]	Loss: 4.6026	Prec@5: 18.433	lr: 0.0979
2025-06-25 15:47:32.274 | INFO     | utils.common:log:12 - Epoch: [0][280/391]	Loss: 4.5675	Prec@5: 18.989	lr: 0.0977
2025-06-25 15:47:32.930 | INFO     | utils.common:log:12 - Epoch: [0][290/391]	Loss: 4.5395	Prec@5: 19.384	lr: 0.0974
2025-06-25 15:47:33.595 | INFO     | utils.common:log:12 - Epoch: [0][300/391]	Loss: 4.5107	Prec@5: 19.882	lr: 0.0971
2025-06-25 15:47:34.265 | INFO     | utils.common:log:12 - Epoch: [0][310/391]	Loss: 4.4839	Prec@5: 20.247	lr: 0.0968
2025-06-25 15:47:34.942 | INFO     | utils.common:log:12 - Epoch: [0][320/391]	Loss: 4.4549	Prec@5: 20.721	lr: 0.0965
2025-06-25 15:47:35.624 | INFO     | utils.common:log:12 - Epoch: [0][330/391]	Loss: 4.4307	Prec@5: 21.120	lr: 0.0962
2025-06-25 15:47:36.316 | INFO     | utils.common:log:12 - Epoch: [0][340/391]	Loss: 4.4057	Prec@5: 21.527	lr: 0.0959
2025-06-25 15:47:37.016 | INFO     | utils.common:log:12 - Epoch: [0][350/391]	Loss: 4.3810	Prec@5: 21.973	lr: 0.0956
2025-06-25 15:47:37.727 | INFO     | utils.common:log:12 - Epoch: [0][360/391]	Loss: 4.3569	Prec@5: 22.416	lr: 0.0952
2025-06-25 15:47:38.449 | INFO     | utils.common:log:12 - Epoch: [0][370/391]	Loss: 4.3366	Prec@5: 22.789	lr: 0.0948
2025-06-25 15:47:39.173 | INFO     | utils.common:log:12 - Epoch: [0][380/391]	Loss: 4.3139	Prec@5: 23.183	lr: 0.0944
2025-06-25 15:47:39.999 | INFO     | utils.common:log:12 - Epoch: [0][390/391]	Loss: 4.2906	Prec@5: 23.640	lr: 0.0940
2025-06-25 15:47:40.252 | INFO     | utils.common:log:12 - Test: [0/40]	Time: 0.203 (0.203)	Loss: 3.6592	Prec@5: 37.500
2025-06-25 15:47:40.388 | INFO     | utils.common:log:12 - Test: [10/40]	Time: 0.013 (0.031)	Loss: 3.7972	Prec@5: 36.790
2025-06-25 15:47:40.523 | INFO     | utils.common:log:12 - Test: [20/40]	Time: 0.014 (0.023)	Loss: 3.7901	Prec@5: 37.091
2025-06-25 15:47:40.658 | INFO     | utils.common:log:12 - Test: [30/40]	Time: 0.013 (0.020)	Loss: 3.8065	Prec@5: 36.643
2025-06-25 15:47:40.881 | INFO     | utils.common:log:12 -  * Prec@5 36.600
2025-06-25 15:47:40.882 | INFO     | utils.common:log:12 - current lr 9.40498e-02
2025-06-25 15:47:41.115 | INFO     | utils.common:log:12 - Epoch: [1][0/391]	Loss: 3.6940	Prec@5: 34.375	lr: 0.0940
2025-06-25 15:47:41.892 | INFO     | utils.common:log:12 - Epoch: [1][10/391]	Loss: 3.5217	Prec@5: 35.795	lr: 0.0936
2025-06-25 15:47:42.690 | INFO     | utils.common:log:12 - Epoch: [1][20/391]	Loss: 3.4805	Prec@5: 37.872	lr: 0.0932
2025-06-25 15:47:43.488 | INFO     | utils.common:log:12 - Epoch: [1][30/391]	Loss: 3.4531	Prec@5: 39.365	lr: 0.0927
2025-06-25 15:47:44.305 | INFO     | utils.common:log:12 - Epoch: [1][40/391]	Loss: 3.4390	Prec@5: 39.329	lr: 0.0923
2025-06-25 15:47:45.119 | INFO     | utils.common:log:12 - Epoch: [1][50/391]	Loss: 3.4358	Prec@5: 39.675	lr: 0.0918
2025-06-25 15:47:45.955 | INFO     | utils.common:log:12 - Epoch: [1][60/391]	Loss: 3.4301	Prec@5: 40.113	lr: 0.0914
2025-06-25 15:47:46.783 | INFO     | utils.common:log:12 - Epoch: [1][70/391]	Loss: 3.4181	Prec@5: 40.581	lr: 0.0909
2025-06-25 15:47:47.637 | INFO     | utils.common:log:12 - Epoch: [1][80/391]	Loss: 3.4046	Prec@5: 40.914	lr: 0.0904
2025-06-25 15:47:48.487 | INFO     | utils.common:log:12 - Epoch: [1][90/391]	Loss: 3.3942	Prec@5: 41.192	lr: 0.0899
2025-06-25 15:47:49.349 | INFO     | utils.common:log:12 - Epoch: [1][100/391]	Loss: 3.3818	Prec@5: 41.538	lr: 0.0894
2025-06-25 15:47:50.199 | INFO     | utils.common:log:12 - Epoch: [1][110/391]	Loss: 3.3757	Prec@5: 41.653	lr: 0.0889
2025-06-25 15:47:51.048 | INFO     | utils.common:log:12 - Epoch: [1][120/391]	Loss: 3.3680	Prec@5: 41.890	lr: 0.0883
2025-06-25 15:47:51.923 | INFO     | utils.common:log:12 - Epoch: [1][130/391]	Loss: 3.3643	Prec@5: 41.913	lr: 0.0878
2025-06-25 15:47:52.856 | INFO     | utils.common:log:12 - Epoch: [1][140/391]	Loss: 3.3586	Prec@5: 41.933	lr: 0.0872
2025-06-25 15:47:53.775 | INFO     | utils.common:log:12 - Epoch: [1][150/391]	Loss: 3.3483	Prec@5: 42.156	lr: 0.0866
2025-06-25 15:47:54.700 | INFO     | utils.common:log:12 - Epoch: [1][160/391]	Loss: 3.3458	Prec@5: 42.333	lr: 0.0861
2025-06-25 15:47:55.596 | INFO     | utils.common:log:12 - Epoch: [1][170/391]	Loss: 3.3390	Prec@5: 42.489	lr: 0.0855
2025-06-25 15:47:56.521 | INFO     | utils.common:log:12 - Epoch: [1][180/391]	Loss: 3.3308	Prec@5: 42.559	lr: 0.0849
2025-06-25 15:47:57.410 | INFO     | utils.common:log:12 - Epoch: [1][190/391]	Loss: 3.3258	Prec@5: 42.826	lr: 0.0842
2025-06-25 15:47:58.292 | INFO     | utils.common:log:12 - Epoch: [1][200/391]	Loss: 3.3213	Prec@5: 43.050	lr: 0.0836
2025-06-25 15:47:59.183 | INFO     | utils.common:log:12 - Epoch: [1][210/391]	Loss: 3.3138	Prec@5: 43.269	lr: 0.0830
2025-06-25 15:48:00.092 | INFO     | utils.common:log:12 - Epoch: [1][220/391]	Loss: 3.3044	Prec@5: 43.623	lr: 0.0824
2025-06-25 15:48:01.004 | INFO     | utils.common:log:12 - Epoch: [1][230/391]	Loss: 3.2958	Prec@5: 43.851	lr: 0.0817
2025-06-25 15:48:01.934 | INFO     | utils.common:log:12 - Epoch: [1][240/391]	Loss: 3.2900	Prec@5: 44.074	lr: 0.0810
2025-06-25 15:48:02.859 | INFO     | utils.common:log:12 - Epoch: [1][250/391]	Loss: 3.2812	Prec@5: 44.254	lr: 0.0804
2025-06-25 15:48:03.801 | INFO     | utils.common:log:12 - Epoch: [1][260/391]	Loss: 3.2728	Prec@5: 44.480	lr: 0.0797
2025-06-25 15:48:04.745 | INFO     | utils.common:log:12 - Epoch: [1][270/391]	Loss: 3.2654	Prec@5: 44.661	lr: 0.0790
2025-06-25 15:48:05.701 | INFO     | utils.common:log:12 - Epoch: [1][280/391]	Loss: 3.2550	Prec@5: 44.990	lr: 0.0783
2025-06-25 15:48:06.666 | INFO     | utils.common:log:12 - Epoch: [1][290/391]	Loss: 3.2500	Prec@5: 45.119	lr: 0.0776
2025-06-25 15:48:07.643 | INFO     | utils.common:log:12 - Epoch: [1][300/391]	Loss: 3.2453	Prec@5: 45.328	lr: 0.0769
2025-06-25 15:48:08.617 | INFO     | utils.common:log:12 - Epoch: [1][310/391]	Loss: 3.2414	Prec@5: 45.438	lr: 0.0762
2025-06-25 15:48:09.594 | INFO     | utils.common:log:12 - Epoch: [1][320/391]	Loss: 3.2311	Prec@5: 45.731	lr: 0.0755
2025-06-25 15:48:10.583 | INFO     | utils.common:log:12 - Epoch: [1][330/391]	Loss: 3.2271	Prec@5: 45.818	lr: 0.0747
2025-06-25 15:48:11.584 | INFO     | utils.common:log:12 - Epoch: [1][340/391]	Loss: 3.2198	Prec@5: 46.009	lr: 0.0740
2025-06-25 15:48:12.601 | INFO     | utils.common:log:12 - Epoch: [1][350/391]	Loss: 3.2134	Prec@5: 46.216	lr: 0.0732
2025-06-25 15:48:13.609 | INFO     | utils.common:log:12 - Epoch: [1][360/391]	Loss: 3.2076	Prec@5: 46.395	lr: 0.0725
2025-06-25 15:48:14.625 | INFO     | utils.common:log:12 - Epoch: [1][370/391]	Loss: 3.2042	Prec@5: 46.504	lr: 0.0717
2025-06-25 15:48:15.664 | INFO     | utils.common:log:12 - Epoch: [1][380/391]	Loss: 3.1990	Prec@5: 46.617	lr: 0.0710
2025-06-25 15:48:16.697 | INFO     | utils.common:log:12 - Epoch: [1][390/391]	Loss: 3.1913	Prec@5: 46.840	lr: 0.0702
2025-06-25 15:48:16.925 | INFO     | utils.common:log:12 - Test: [0/40]	Time: 0.164 (0.164)	Loss: 3.0932	Prec@5: 57.031
2025-06-25 15:48:17.063 | INFO     | utils.common:log:12 - Test: [10/40]	Time: 0.014 (0.028)	Loss: 3.2793	Prec@5: 49.858
2025-06-25 15:48:17.199 | INFO     | utils.common:log:12 - Test: [20/40]	Time: 0.013 (0.021)	Loss: 3.2732	Prec@5: 49.963
2025-06-25 15:48:17.333 | INFO     | utils.common:log:12 - Test: [30/40]	Time: 0.013 (0.018)	Loss: 3.3067	Prec@5: 49.269
2025-06-25 15:48:17.506 | INFO     | utils.common:log:12 -  * Prec@5 49.200
2025-06-25 15:48:17.506 | INFO     | utils.common:log:12 - current lr 7.01949e-02
2025-06-25 15:48:17.786 | INFO     | utils.common:log:12 - Epoch: [2][0/391]	Loss: 3.0506	Prec@5: 57.812	lr: 0.0701
2025-06-25 15:48:18.831 | INFO     | utils.common:log:12 - Epoch: [2][10/391]	Loss: 3.0024	Prec@5: 52.699	lr: 0.0693
2025-06-25 15:48:19.897 | INFO     | utils.common:log:12 - Epoch: [2][20/391]	Loss: 2.9691	Prec@5: 54.911	lr: 0.0686
2025-06-25 15:48:20.966 | INFO     | utils.common:log:12 - Epoch: [2][30/391]	Loss: 2.9336	Prec@5: 55.796	lr: 0.0678
2025-06-25 15:48:22.033 | INFO     | utils.common:log:12 - Epoch: [2][40/391]	Loss: 2.9206	Prec@5: 55.945	lr: 0.0670
2025-06-25 15:48:23.118 | INFO     | utils.common:log:12 - Epoch: [2][50/391]	Loss: 2.9263	Prec@5: 55.515	lr: 0.0662
2025-06-25 15:48:24.195 | INFO     | utils.common:log:12 - Epoch: [2][60/391]	Loss: 2.9269	Prec@5: 55.405	lr: 0.0654
2025-06-25 15:48:25.279 | INFO     | utils.common:log:12 - Epoch: [2][70/391]	Loss: 2.9206	Prec@5: 55.392	lr: 0.0646
2025-06-25 15:48:26.383 | INFO     | utils.common:log:12 - Epoch: [2][80/391]	Loss: 2.9120	Prec@5: 55.633	lr: 0.0638
2025-06-25 15:48:27.491 | INFO     | utils.common:log:12 - Epoch: [2][90/391]	Loss: 2.9086	Prec@5: 55.529	lr: 0.0629
2025-06-25 15:48:28.593 | INFO     | utils.common:log:12 - Epoch: [2][100/391]	Loss: 2.8973	Prec@5: 55.925	lr: 0.0621
2025-06-25 15:48:29.712 | INFO     | utils.common:log:12 - Epoch: [2][110/391]	Loss: 2.8905	Prec@5: 56.123	lr: 0.0613
2025-06-25 15:48:30.851 | INFO     | utils.common:log:12 - Epoch: [2][120/391]	Loss: 2.8820	Prec@5: 56.418	lr: 0.0605
2025-06-25 15:48:31.995 | INFO     | utils.common:log:12 - Epoch: [2][130/391]	Loss: 2.8813	Prec@5: 56.512	lr: 0.0596
2025-06-25 15:48:33.141 | INFO     | utils.common:log:12 - Epoch: [2][140/391]	Loss: 2.8815	Prec@5: 56.438	lr: 0.0588
2025-06-25 15:48:34.286 | INFO     | utils.common:log:12 - Epoch: [2][150/391]	Loss: 2.8794	Prec@5: 56.364	lr: 0.0580
2025-06-25 15:48:35.437 | INFO     | utils.common:log:12 - Epoch: [2][160/391]	Loss: 2.8811	Prec@5: 56.308	lr: 0.0571
2025-06-25 15:48:36.596 | INFO     | utils.common:log:12 - Epoch: [2][170/391]	Loss: 2.8792	Prec@5: 56.305	lr: 0.0563
2025-06-25 15:48:37.765 | INFO     | utils.common:log:12 - Epoch: [2][180/391]	Loss: 2.8747	Prec@5: 56.276	lr: 0.0555
2025-06-25 15:48:38.952 | INFO     | utils.common:log:12 - Epoch: [2][190/391]	Loss: 2.8720	Prec@5: 56.455	lr: 0.0546
2025-06-25 15:48:40.159 | INFO     | utils.common:log:12 - Epoch: [2][200/391]	Loss: 2.8708	Prec@5: 56.615	lr: 0.0538
2025-06-25 15:48:41.368 | INFO     | utils.common:log:12 - Epoch: [2][210/391]	Loss: 2.8659	Prec@5: 56.768	lr: 0.0529
2025-06-25 15:48:42.595 | INFO     | utils.common:log:12 - Epoch: [2][220/391]	Loss: 2.8586	Prec@5: 57.014	lr: 0.0521
2025-06-25 15:48:43.879 | INFO     | utils.common:log:12 - Epoch: [2][230/391]	Loss: 2.8511	Prec@5: 57.210	lr: 0.0512
2025-06-25 15:48:45.178 | INFO     | utils.common:log:12 - Epoch: [2][240/391]	Loss: 2.8496	Prec@5: 57.216	lr: 0.0504
2025-06-25 15:48:46.451 | INFO     | utils.common:log:12 - Epoch: [2][250/391]	Loss: 2.8433	Prec@5: 57.339	lr: 0.0495
2025-06-25 15:48:47.742 | INFO     | utils.common:log:12 - Epoch: [2][260/391]	Loss: 2.8371	Prec@5: 57.441	lr: 0.0487
2025-06-25 15:48:49.043 | INFO     | utils.common:log:12 - Epoch: [2][270/391]	Loss: 2.8308	Prec@5: 57.588	lr: 0.0478
2025-06-25 15:48:50.343 | INFO     | utils.common:log:12 - Epoch: [2][280/391]	Loss: 2.8224	Prec@5: 57.785	lr: 0.0470
2025-06-25 15:48:51.665 | INFO     | utils.common:log:12 - Epoch: [2][290/391]	Loss: 2.8189	Prec@5: 57.791	lr: 0.0462
2025-06-25 15:48:52.978 | INFO     | utils.common:log:12 - Epoch: [2][300/391]	Loss: 2.8165	Prec@5: 57.885	lr: 0.0453
2025-06-25 15:48:54.310 | INFO     | utils.common:log:12 - Epoch: [2][310/391]	Loss: 2.8131	Prec@5: 57.988	lr: 0.0445
2025-06-25 15:48:55.636 | INFO     | utils.common:log:12 - Epoch: [2][320/391]	Loss: 2.8044	Prec@5: 58.255	lr: 0.0436
2025-06-25 15:48:56.990 | INFO     | utils.common:log:12 - Epoch: [2][330/391]	Loss: 2.8024	Prec@5: 58.294	lr: 0.0428
2025-06-25 15:48:58.333 | INFO     | utils.common:log:12 - Epoch: [2][340/391]	Loss: 2.7949	Prec@5: 58.459	lr: 0.0419
2025-06-25 15:48:59.687 | INFO     | utils.common:log:12 - Epoch: [2][350/391]	Loss: 2.7883	Prec@5: 58.667	lr: 0.0411
2025-06-25 15:49:01.052 | INFO     | utils.common:log:12 - Epoch: [2][360/391]	Loss: 2.7836	Prec@5: 58.791	lr: 0.0403
2025-06-25 15:49:02.439 | INFO     | utils.common:log:12 - Epoch: [2][370/391]	Loss: 2.7829	Prec@5: 58.819	lr: 0.0395
2025-06-25 15:49:03.817 | INFO     | utils.common:log:12 - Epoch: [2][380/391]	Loss: 2.7789	Prec@5: 58.879	lr: 0.0386
2025-06-25 15:49:05.195 | INFO     | utils.common:log:12 - Epoch: [2][390/391]	Loss: 2.7735	Prec@5: 58.992	lr: 0.0378
2025-06-25 15:49:05.430 | INFO     | utils.common:log:12 - Test: [0/40]	Time: 0.167 (0.167)	Loss: 2.5629	Prec@5: 64.062
2025-06-25 15:49:05.565 | INFO     | utils.common:log:12 - Test: [10/40]	Time: 0.013 (0.027)	Loss: 2.8980	Prec@5: 58.736
2025-06-25 15:49:05.699 | INFO     | utils.common:log:12 - Test: [20/40]	Time: 0.013 (0.021)	Loss: 2.8902	Prec@5: 58.854
2025-06-25 15:49:05.833 | INFO     | utils.common:log:12 - Test: [30/40]	Time: 0.013 (0.018)	Loss: 2.9130	Prec@5: 58.291
2025-06-25 15:49:06.002 | INFO     | utils.common:log:12 -  * Prec@5 57.800
2025-06-25 15:49:06.002 | INFO     | utils.common:log:12 - current lr 3.78035e-02
2025-06-25 15:49:06.325 | INFO     | utils.common:log:12 - Epoch: [3][0/391]	Loss: 2.5682	Prec@5: 64.062	lr: 0.0377
2025-06-25 15:49:07.790 | INFO     | utils.common:log:12 - Epoch: [3][10/391]	Loss: 2.5987	Prec@5: 62.784	lr: 0.0369
2025-06-25 15:49:09.202 | INFO     | utils.common:log:12 - Epoch: [3][20/391]	Loss: 2.5874	Prec@5: 63.170	lr: 0.0361
2025-06-25 15:49:10.607 | INFO     | utils.common:log:12 - Epoch: [3][30/391]	Loss: 2.5617	Prec@5: 63.911	lr: 0.0353
2025-06-25 15:49:12.017 | INFO     | utils.common:log:12 - Epoch: [3][40/391]	Loss: 2.5478	Prec@5: 64.405	lr: 0.0345
2025-06-25 15:49:13.435 | INFO     | utils.common:log:12 - Epoch: [3][50/391]	Loss: 2.5656	Prec@5: 63.787	lr: 0.0337
2025-06-25 15:49:14.888 | INFO     | utils.common:log:12 - Epoch: [3][60/391]	Loss: 2.5556	Prec@5: 63.934	lr: 0.0329
2025-06-25 15:49:16.323 | INFO     | utils.common:log:12 - Epoch: [3][70/391]	Loss: 2.5531	Prec@5: 63.864	lr: 0.0321
2025-06-25 15:49:17.774 | INFO     | utils.common:log:12 - Epoch: [3][80/391]	Loss: 2.5481	Prec@5: 63.947	lr: 0.0313
2025-06-25 15:49:19.237 | INFO     | utils.common:log:12 - Epoch: [3][90/391]	Loss: 2.5475	Prec@5: 63.959	lr: 0.0305
2025-06-25 15:49:20.705 | INFO     | utils.common:log:12 - Epoch: [3][100/391]	Loss: 2.5374	Prec@5: 64.465	lr: 0.0297
2025-06-25 15:49:22.170 | INFO     | utils.common:log:12 - Epoch: [3][110/391]	Loss: 2.5365	Prec@5: 64.668	lr: 0.0290
2025-06-25 15:49:23.650 | INFO     | utils.common:log:12 - Epoch: [3][120/391]	Loss: 2.5313	Prec@5: 64.850	lr: 0.0282
2025-06-25 15:49:25.136 | INFO     | utils.common:log:12 - Epoch: [3][130/391]	Loss: 2.5306	Prec@5: 64.933	lr: 0.0274
2025-06-25 15:49:26.628 | INFO     | utils.common:log:12 - Epoch: [3][140/391]	Loss: 2.5335	Prec@5: 64.772	lr: 0.0267
2025-06-25 15:49:28.132 | INFO     | utils.common:log:12 - Epoch: [3][150/391]	Loss: 2.5294	Prec@5: 64.776	lr: 0.0259
2025-06-25 15:49:29.639 | INFO     | utils.common:log:12 - Epoch: [3][160/391]	Loss: 2.5328	Prec@5: 64.761	lr: 0.0252
2025-06-25 15:49:31.154 | INFO     | utils.common:log:12 - Epoch: [3][170/391]	Loss: 2.5297	Prec@5: 64.784	lr: 0.0245
2025-06-25 15:49:32.681 | INFO     | utils.common:log:12 - Epoch: [3][180/391]	Loss: 2.5271	Prec@5: 64.814	lr: 0.0237
2025-06-25 15:49:34.226 | INFO     | utils.common:log:12 - Epoch: [3][190/391]	Loss: 2.5249	Prec@5: 64.840	lr: 0.0230
2025-06-25 15:49:35.790 | INFO     | utils.common:log:12 - Epoch: [3][200/391]	Loss: 2.5239	Prec@5: 64.918	lr: 0.0223
2025-06-25 15:49:37.377 | INFO     | utils.common:log:12 - Epoch: [3][210/391]	Loss: 2.5199	Prec@5: 65.084	lr: 0.0216
2025-06-25 15:49:38.957 | INFO     | utils.common:log:12 - Epoch: [3][220/391]	Loss: 2.5147	Prec@5: 65.328	lr: 0.0209
2025-06-25 15:49:40.553 | INFO     | utils.common:log:12 - Epoch: [3][230/391]	Loss: 2.5085	Prec@5: 65.483	lr: 0.0202
2025-06-25 15:49:42.141 | INFO     | utils.common:log:12 - Epoch: [3][240/391]	Loss: 2.5058	Prec@5: 65.489	lr: 0.0196
2025-06-25 15:49:43.739 | INFO     | utils.common:log:12 - Epoch: [3][250/391]	Loss: 2.5032	Prec@5: 65.575	lr: 0.0189
2025-06-25 15:49:45.334 | INFO     | utils.common:log:12 - Epoch: [3][260/391]	Loss: 2.4967	Prec@5: 65.769	lr: 0.0182
2025-06-25 15:49:46.942 | INFO     | utils.common:log:12 - Epoch: [3][270/391]	Loss: 2.4925	Prec@5: 65.861	lr: 0.0176
2025-06-25 15:49:48.553 | INFO     | utils.common:log:12 - Epoch: [3][280/391]	Loss: 2.4844	Prec@5: 66.098	lr: 0.0169
2025-06-25 15:49:50.178 | INFO     | utils.common:log:12 - Epoch: [3][290/391]	Loss: 2.4803	Prec@5: 66.189	lr: 0.0163
2025-06-25 15:49:51.806 | INFO     | utils.common:log:12 - Epoch: [3][300/391]	Loss: 2.4773	Prec@5: 66.206	lr: 0.0157
2025-06-25 15:49:53.451 | INFO     | utils.common:log:12 - Epoch: [3][310/391]	Loss: 2.4748	Prec@5: 66.273	lr: 0.0151
2025-06-25 15:49:55.100 | INFO     | utils.common:log:12 - Epoch: [3][320/391]	Loss: 2.4682	Prec@5: 66.472	lr: 0.0145
2025-06-25 15:49:56.774 | INFO     | utils.common:log:12 - Epoch: [3][330/391]	Loss: 2.4661	Prec@5: 66.475	lr: 0.0139
2025-06-25 15:49:58.444 | INFO     | utils.common:log:12 - Epoch: [3][340/391]	Loss: 2.4603	Prec@5: 66.628	lr: 0.0133
2025-06-25 15:50:00.134 | INFO     | utils.common:log:12 - Epoch: [3][350/391]	Loss: 2.4541	Prec@5: 66.778	lr: 0.0127
2025-06-25 15:50:01.830 | INFO     | utils.common:log:12 - Epoch: [3][360/391]	Loss: 2.4510	Prec@5: 66.850	lr: 0.0122
2025-06-25 15:50:03.541 | INFO     | utils.common:log:12 - Epoch: [3][370/391]	Loss: 2.4500	Prec@5: 66.918	lr: 0.0116
2025-06-25 15:50:05.271 | INFO     | utils.common:log:12 - Epoch: [3][380/391]	Loss: 2.4477	Prec@5: 66.954	lr: 0.0111
2025-06-25 15:50:07.000 | INFO     | utils.common:log:12 - Epoch: [3][390/391]	Loss: 2.4434	Prec@5: 67.020	lr: 0.0106
2025-06-25 15:50:07.234 | INFO     | utils.common:log:12 - Test: [0/40]	Time: 0.175 (0.175)	Loss: 2.1027	Prec@5: 76.562
2025-06-25 15:50:07.370 | INFO     | utils.common:log:12 - Test: [10/40]	Time: 0.013 (0.028)	Loss: 2.4406	Prec@5: 67.259
2025-06-25 15:50:07.503 | INFO     | utils.common:log:12 - Test: [20/40]	Time: 0.013 (0.021)	Loss: 2.4628	Prec@5: 66.778
2025-06-25 15:50:07.635 | INFO     | utils.common:log:12 - Test: [30/40]	Time: 0.013 (0.019)	Loss: 2.4649	Prec@5: 66.709
2025-06-25 15:50:07.802 | INFO     | utils.common:log:12 -  * Prec@5 66.100
2025-06-25 15:50:07.802 | INFO     | utils.common:log:12 - current lr 1.05676e-02
2025-06-25 15:50:08.162 | INFO     | utils.common:log:12 - Epoch: [4][0/391]	Loss: 2.2414	Prec@5: 73.438	lr: 0.0105
2025-06-25 15:50:10.049 | INFO     | utils.common:log:12 - Epoch: [4][10/391]	Loss: 2.3277	Prec@5: 71.591	lr: 0.0100
2025-06-25 15:50:11.785 | INFO     | utils.common:log:12 - Epoch: [4][20/391]	Loss: 2.3203	Prec@5: 70.982	lr: 0.0095
2025-06-25 15:50:13.546 | INFO     | utils.common:log:12 - Epoch: [4][30/391]	Loss: 2.2802	Prec@5: 71.472	lr: 0.0090
2025-06-25 15:50:15.311 | INFO     | utils.common:log:12 - Epoch: [4][40/391]	Loss: 2.2905	Prec@5: 71.037	lr: 0.0085
2025-06-25 15:50:17.085 | INFO     | utils.common:log:12 - Epoch: [4][50/391]	Loss: 2.3075	Prec@5: 70.588	lr: 0.0081
2025-06-25 15:50:18.872 | INFO     | utils.common:log:12 - Epoch: [4][60/391]	Loss: 2.3093	Prec@5: 70.594	lr: 0.0076
2025-06-25 15:50:20.670 | INFO     | utils.common:log:12 - Epoch: [4][70/391]	Loss: 2.3072	Prec@5: 70.709	lr: 0.0072
2025-06-25 15:50:22.486 | INFO     | utils.common:log:12 - Epoch: [4][80/391]	Loss: 2.3099	Prec@5: 70.486	lr: 0.0067
2025-06-25 15:50:24.344 | INFO     | utils.common:log:12 - Epoch: [4][90/391]	Loss: 2.3051	Prec@5: 70.553	lr: 0.0063
2025-06-25 15:50:26.152 | INFO     | utils.common:log:12 - Epoch: [4][100/391]	Loss: 2.2981	Prec@5: 70.560	lr: 0.0059
2025-06-25 15:50:27.953 | INFO     | utils.common:log:12 - Epoch: [4][110/391]	Loss: 2.2970	Prec@5: 70.552	lr: 0.0055
2025-06-25 15:50:29.767 | INFO     | utils.common:log:12 - Epoch: [4][120/391]	Loss: 2.2902	Prec@5: 70.790	lr: 0.0051
2025-06-25 15:50:31.582 | INFO     | utils.common:log:12 - Epoch: [4][130/391]	Loss: 2.2939	Prec@5: 70.825	lr: 0.0048
2025-06-25 15:50:33.412 | INFO     | utils.common:log:12 - Epoch: [4][140/391]	Loss: 2.2958	Prec@5: 70.745	lr: 0.0044
2025-06-25 15:50:35.236 | INFO     | utils.common:log:12 - Epoch: [4][150/391]	Loss: 2.2940	Prec@5: 70.706	lr: 0.0041
2025-06-25 15:50:37.090 | INFO     | utils.common:log:12 - Epoch: [4][160/391]	Loss: 2.3002	Prec@5: 70.633	lr: 0.0037
2025-06-25 15:50:38.941 | INFO     | utils.common:log:12 - Epoch: [4][170/391]	Loss: 2.2995	Prec@5: 70.641	lr: 0.0034
2025-06-25 15:50:40.803 | INFO     | utils.common:log:12 - Epoch: [4][180/391]	Loss: 2.2993	Prec@5: 70.623	lr: 0.0031
2025-06-25 15:50:42.674 | INFO     | utils.common:log:12 - Epoch: [4][190/391]	Loss: 2.3013	Prec@5: 70.623	lr: 0.0028
2025-06-25 15:50:44.555 | INFO     | utils.common:log:12 - Epoch: [4][200/391]	Loss: 2.3003	Prec@5: 70.701	lr: 0.0026
2025-06-25 15:50:46.452 | INFO     | utils.common:log:12 - Epoch: [4][210/391]	Loss: 2.2996	Prec@5: 70.772	lr: 0.0023
2025-06-25 15:50:48.368 | INFO     | utils.common:log:12 - Epoch: [4][220/391]	Loss: 2.2983	Prec@5: 70.913	lr: 0.0021
2025-06-25 15:50:50.276 | INFO     | utils.common:log:12 - Epoch: [4][230/391]	Loss: 2.2946	Prec@5: 71.084	lr: 0.0018
2025-06-25 15:50:52.202 | INFO     | utils.common:log:12 - Epoch: [4][240/391]	Loss: 2.2967	Prec@5: 71.084	lr: 0.0016
2025-06-25 15:50:54.119 | INFO     | utils.common:log:12 - Epoch: [4][250/391]	Loss: 2.2969	Prec@5: 71.122	lr: 0.0014
2025-06-25 15:50:56.060 | INFO     | utils.common:log:12 - Epoch: [4][260/391]	Loss: 2.2932	Prec@5: 71.300	lr: 0.0012
2025-06-25 15:50:58.014 | INFO     | utils.common:log:12 - Epoch: [4][270/391]	Loss: 2.2917	Prec@5: 71.333	lr: 0.0010
2025-06-25 15:50:59.960 | INFO     | utils.common:log:12 - Epoch: [4][280/391]	Loss: 2.2900	Prec@5: 71.402	lr: 0.0009
2025-06-25 15:51:01.914 | INFO     | utils.common:log:12 - Epoch: [4][290/391]	Loss: 2.2905	Prec@5: 71.413	lr: 0.0007
2025-06-25 15:51:03.877 | INFO     | utils.common:log:12 - Epoch: [4][300/391]	Loss: 2.2915	Prec@5: 71.408	lr: 0.0006
2025-06-25 15:51:05.855 | INFO     | utils.common:log:12 - Epoch: [4][310/391]	Loss: 2.2961	Prec@5: 71.302	lr: 0.0005
2025-06-25 15:51:07.844 | INFO     | utils.common:log:12 - Epoch: [4][320/391]	Loss: 2.2961	Prec@5: 71.340	lr: 0.0004
2025-06-25 15:51:09.820 | INFO     | utils.common:log:12 - Epoch: [4][330/391]	Loss: 2.3007	Prec@5: 71.266	lr: 0.0003
2025-06-25 15:51:11.821 | INFO     | utils.common:log:12 - Epoch: [4][340/391]	Loss: 2.3027	Prec@5: 71.270	lr: 0.0002
2025-06-25 15:51:13.827 | INFO     | utils.common:log:12 - Epoch: [4][350/391]	Loss: 2.3043	Prec@5: 71.256	lr: 0.0001
2025-06-25 15:51:15.836 | INFO     | utils.common:log:12 - Epoch: [4][360/391]	Loss: 2.3105	Prec@5: 71.152	lr: 0.0001
2025-06-25 15:51:17.867 | INFO     | utils.common:log:12 - Epoch: [4][370/391]	Loss: 2.3192	Prec@5: 70.999	lr: 0.0000
2025-06-25 15:51:19.892 | INFO     | utils.common:log:12 - Epoch: [4][380/391]	Loss: 2.3253	Prec@5: 70.887	lr: 0.0000
2025-06-25 15:51:21.916 | INFO     | utils.common:log:12 - Epoch: [4][390/391]	Loss: 2.3321	Prec@5: 70.780	lr: 0.0000
2025-06-25 15:51:22.148 | INFO     | utils.common:log:12 - Test: [0/40]	Time: 0.168 (0.168)	Loss: 2.3819	Prec@5: 70.312
2025-06-25 15:51:22.284 | INFO     | utils.common:log:12 - Test: [10/40]	Time: 0.013 (0.028)	Loss: 2.6950	Prec@5: 63.281
2025-06-25 15:51:22.447 | INFO     | utils.common:log:12 - Test: [20/40]	Time: 0.019 (0.022)	Loss: 2.7239	Prec@5: 62.128
2025-06-25 15:51:22.581 | INFO     | utils.common:log:12 - Test: [30/40]	Time: 0.013 (0.019)	Loss: 2.7277	Prec@5: 62.223
2025-06-25 15:51:22.781 | INFO     | utils.common:log:12 -  * Prec@5 61.620
