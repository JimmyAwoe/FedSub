W0630 15:55:50.440000 1233110 site-packages/torch/distributed/run.py:766] 
W0630 15:55:50.440000 1233110 site-packages/torch/distributed/run.py:766] *****************************************
W0630 15:55:50.440000 1233110 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0630 15:55:50.440000 1233110 site-packages/torch/distributed/run.py:766] *****************************************
2025-06-30 15:55:55.071 | INFO     | utils.common:log:12 - Process group initialize
2025-06-30 15:55:55.071 | INFO     | utils.common:log:12 - ****************************************
2025-06-30 15:55:55.071 | INFO     | utils.common:log:12 - Start training with arguments
2025-06-30 15:55:55.071 | INFO     | utils.common:log:12 - lr                             0.001
2025-06-30 15:55:55.071 | INFO     | utils.common:log:12 - batch_size                     64
2025-06-30 15:55:55.071 | INFO     | utils.common:log:12 - total_batch_size               64
2025-06-30 15:55:55.071 | INFO     | utils.common:log:12 - max_length                     1024
2025-06-30 15:55:55.071 | INFO     | utils.common:log:12 - num_training_steps             20
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - grad_clip                      0.0
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - warmup                         1000
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - constant_lr                    True
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - seed                           42
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - mixed_precision                bf16
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - comp_dim                       4096
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - tau                            10
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - gene_method                    idx
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - jump_certain_modules           False
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - update_cp_freq                 10
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - adaptive_cp_rate               1.0
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - model_config                   configs/llama_60m.json
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - optimizer                      fedavgsgd
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - momentum                       0.0
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - dampening                      0
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - weight_decay                   0
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - nesterov                       False
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - per_layer_weight_update        False
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - use_wandb                      False
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - wandb_run_name                 real_lazy_update
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - use_tqdm                       False
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - use_log                        True
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - mem_monitor                    False
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - flash_attn                     False
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - ckpt                           True
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - measure_comm                   True
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - change_cd                      3000
2025-06-30 15:55:55.072 | INFO     | utils.common:log:12 - ****************************************
/home/xyq/miniconda3/envs/subscaf/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation
  warnings.warn(
/home/xyq/miniconda3/envs/subscaf/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation
  warnings.warn(
2025-06-30 15:55:58.994 | INFO     | utils.common:log:12 - 
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 512, padding_idx=31999)
    (layers): ModuleList(
      (0-7): 8 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=512, out_features=512, bias=False)
          (k_proj): Linear(in_features=512, out_features=512, bias=False)
          (v_proj): Linear(in_features=512, out_features=512, bias=False)
          (o_proj): Linear(in_features=512, out_features=512, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=512, out_features=1376, bias=False)
          (up_proj): Linear(in_features=512, out_features=1376, bias=False)
          (down_proj): Linear(in_features=1376, out_features=512, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((512,), eps=1e-06)
        (post_attention_layernorm): LlamaRMSNorm((512,), eps=1e-06)
      )
    )
    (norm): LlamaRMSNorm((512,), eps=1e-06)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=512, out_features=32000, bias=False)
)

2025-06-30 15:55:58.994 | INFO     | utils.common:log:12 - Total params: 58.07M
2025-06-30 15:55:58.995 | INFO     | utils.common:log:12 - Trainable params: 58.07M
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/xyq/gitee-SubScaf/llama_pretrain.py", line 427, in <module>
[rank0]:     main(args)
[rank0]:   File "/home/xyq/gitee-SubScaf/llama_pretrain.py", line 206, in main
[rank0]:     update_time = time.time()
[rank0]: UnboundLocalError: local variable 'time' referenced before assignment
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/xyq/gitee-SubScaf/llama_pretrain.py", line 427, in <module>
[rank1]:     main(args)
[rank1]:   File "/home/xyq/gitee-SubScaf/llama_pretrain.py", line 206, in main
[rank1]:     update_time = time.time()
[rank1]: UnboundLocalError: local variable 'time' referenced before assignment
[rank0]:[W630 15:55:59.777639824 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W0630 15:56:00.671000 1233110 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1233258 closing signal SIGTERM
E0630 15:56:00.836000 1233110 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 1233257) of binary: /home/xyq/miniconda3/envs/subscaf/bin/python3.10
Traceback (most recent call last):
  File "/home/xyq/miniconda3/envs/subscaf/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/xyq/miniconda3/envs/subscaf/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/xyq/miniconda3/envs/subscaf/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in main
    run(args)
  File "/home/xyq/miniconda3/envs/subscaf/lib/python3.10/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/home/xyq/miniconda3/envs/subscaf/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xyq/miniconda3/envs/subscaf/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
llama_pretrain.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-06-30_15:56:00
  host      : ubuntu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1233257)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
